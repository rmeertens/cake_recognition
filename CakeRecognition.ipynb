{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage.color import gray2rgb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# infile = 'food/meta/train.json'\n",
    "# with open(infile) as json_file:  \n",
    "#     data = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import numpy as np\n",
    "\n",
    "# random example images\n",
    "images = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8)\n",
    "\n",
    "# Sometimes(0.5, ...) applies the given augmenter in 50% of all cases,\n",
    "# e.g. Sometimes(0.5, GaussianBlur(0.3)) would blur roughly every second image.\n",
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "\n",
    "# Define our sequence of augmentation steps that will be applied to every image\n",
    "# All augmenters with per_channel=0.5 will sample one value _per image_\n",
    "# in 50% of all cases. In all other cases they will sample new values\n",
    "# _per channel_.\n",
    "seq = iaa.Sequential(\n",
    "    [\n",
    "        # apply the following augmenters to most images\n",
    "        iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
    "        iaa.Flipud(0.2), # vertically flip 20% of all images\n",
    "        # crop images by -5% to 10% of their height/width\n",
    "        sometimes(iaa.CropAndPad(\n",
    "            percent=(-0.05, 0.1),\n",
    "            pad_mode=ia.ALL,\n",
    "            pad_cval=(0, 255)\n",
    "        )),\n",
    "        sometimes(iaa.Affine(\n",
    "            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis\n",
    "            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, # translate by -20 to +20 percent (per axis)\n",
    "            rotate=(-45, 45), # rotate by -45 to +45 degrees\n",
    "            shear=(-16, 16), # shear by -16 to +16 degrees\n",
    "            order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n",
    "            cval=(0, 255), # if mode is constant, use a cval between 0 and 255\n",
    "            mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
    "        )),\n",
    "        # execute 0 to 5 of the following (less important) augmenters per image\n",
    "        # don't execute all of them, as that would often be way too strong\n",
    "        iaa.SomeOf((0, 5),\n",
    "            [\n",
    "#                 sometimes(iaa.Superpixels(p_replace=(0, 1.0), n_segments=(20, 200))), # convert images into their superpixel representation\n",
    "#                 iaa.OneOf([\n",
    "#                     iaa.GaussianBlur((0, 3.0)), # blur images with a sigma between 0 and 3.0\n",
    "#                     iaa.AverageBlur(k=(2, 7)), # blur image using local means with kernel sizes between 2 and 7\n",
    "#                     iaa.MedianBlur(k=(3, 11)), # blur image using local medians with kernel sizes between 2 and 7\n",
    "#                 ]),\n",
    "                iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images\n",
    "                iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images\n",
    "                # search either for all edges or for directed edges,\n",
    "                # blend the result with the original image using a blobby mask\n",
    "                iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
    "                    iaa.EdgeDetect(alpha=(0.5, 1.0)),\n",
    "                    iaa.DirectedEdgeDetect(alpha=(0.5, 1.0), direction=(0.0, 1.0)),\n",
    "                ])),\n",
    "                iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5), # add gaussian noise to images\n",
    "                iaa.OneOf([\n",
    "                    iaa.Dropout((0.01, 0.1), per_channel=0.5), # randomly remove up to 10% of the pixels\n",
    "                    iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05), per_channel=0.2),\n",
    "                ]),\n",
    "                iaa.Invert(0.05, per_channel=True), # invert color channels\n",
    "#                 iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value)\n",
    "#                 iaa.AddToHueAndSaturation((-20, 20)), # change hue and saturation\n",
    "#                 # either change the brightness of the whole image (sometimes\n",
    "#                 # per channel) or change the brightness of subareas\n",
    "#                 iaa.OneOf([\n",
    "#                     iaa.Multiply((0.5, 1.5), per_channel=0.5),\n",
    "#                     iaa.FrequencyNoiseAlpha(\n",
    "#                         exponent=(-4, 0),\n",
    "#                         first=iaa.Multiply((0.5, 1.5), per_channel=True),\n",
    "#                         second=iaa.ContrastNormalization((0.5, 2.0))\n",
    "#                     )\n",
    "#                 ]),\n",
    "                iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast\n",
    "#                 iaa.Grayscale(alpha=(0.0, 1.0)),\n",
    "#                 sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)), # move pixels locally around (with random strengths)\n",
    "#                 sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))), # sometimes move parts of the image around\n",
    "                sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n",
    "            ],\n",
    "            random_order=True\n",
    "        )\n",
    "    ],\n",
    "    random_order=True\n",
    ")\n",
    "\n",
    "images_aug = seq.augment_images(images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 101\n",
      "filenames len:  75750  labels len:  75750\n",
      "Number of classes: 101\n",
      "filenames len:  25250  labels len:  25250\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, infile, batch_size=128, is_training = True):\n",
    "        with open(infile) as json_file:  \n",
    "            data = json.load(json_file)\n",
    "#         newdata = dict()\n",
    "#         for name in data.keys():\n",
    "#             if name[0] in 'abcd':\n",
    "#                 newdata[name] = data[name]                \n",
    "#         data = newdata\n",
    "        self.num_classes = len(data.keys())\n",
    "        self.is_training = is_training\n",
    "        print(\"Number of classes: \" + str(self.num_classes))\n",
    "\n",
    "        self.batch_size=batch_size\n",
    "\n",
    "        self.labels = list()\n",
    "        \n",
    "        self.filenames = list()\n",
    "        self.label_names = sorted(data.keys())\n",
    "        for index, key in enumerate(self.label_names):\n",
    "            for image in data[key]:\n",
    "                self.filenames.append(image)\n",
    "                self.labels.append(index)\n",
    "        print(\"filenames len: \", len(self.filenames), \" labels len: \", len(self.labels))\n",
    "        self.num_images = len(self.filenames)            \n",
    "        self.indices = np.arange(self.num_images)\n",
    "        self.on_epoch_end()\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch\n",
    "        return int(np.floor(self.num_images / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data...\"\"\"\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        images = list()\n",
    "        labels = list()\n",
    "        for i in indexes:\n",
    "            filename = self.filenames[i]\n",
    "            label = self.labels[i]            \n",
    "            basepath = 'food/images/'\n",
    "            path = basepath + filename + \".jpg\"\n",
    "            img = tf.keras.preprocessing.image.load_img(path, target_size=(224, 224))\n",
    "            x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "            assert(x.shape == (224, 224, 3))\n",
    "\n",
    "            images.append(x)\n",
    "            labels.append(label)\n",
    "            \n",
    "        if self.is_training:\n",
    "            images = seq.augment_images(images)\n",
    "\n",
    "        temp = np.array(images)\n",
    "\n",
    "#         unresized_images = tf.keras.applications.resnet50.preprocess_input(temp)\n",
    "        unresized_images = tf.keras.applications.mobilenet.preprocess_input(temp)\n",
    "\n",
    "        return np.array(unresized_images), tf.keras.utils.to_categorical(labels, num_classes=self.num_classes)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        np.random.shuffle(self.indices)\n",
    "         \n",
    "train_generator = DataGenerator('food/meta/train.json', is_training=True)\n",
    "test_generator = DataGenerator('food/meta/test.json', is_training=False)\n",
    "\n",
    "firstitem = train_generator.__getitem__(0)\n",
    "\n",
    "# for i in range(1):\n",
    "#     firstitem = train_generator.__getitem__(i)\n",
    "#     for x, label in zip(firstitem[0], firstitem[1]):\n",
    "#         plt.imshow(x)\n",
    "#         plt.show()\n",
    "#         print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "#     featurewise_center=True,\n",
    "#     featurewise_std_normalization=True,\n",
    "#     rotation_range=20,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     horizontal_flip=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen.fit(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_generator.label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstitem = train_generator.__getitem__(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 224, 224, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstitem[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_generator.label_names == train_generator.label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# firstitem[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 224, 224, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 224, 224, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 224, 224, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 224, 224, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model (Model)                   (None, 101)          3336485     lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Concatenate)             (None, 101)          0           model[1][0]                      \n",
      "                                                                 model[2][0]                      \n",
      "                                                                 model[3][0]                      \n",
      "                                                                 model[4][0]                      \n",
      "==================================================================================================\n",
      "Total params: 3,336,485\n",
      "Trainable params: 3,312,549\n",
      "Non-trainable params: 23,936\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# network = tf.keras.applications.resnet50.ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "network = tf.keras.applications.mobilenet.MobileNet(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "x = network.output\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x=tf.keras.layers.Flatten()(x)\n",
    "# x=tf.keras.layers.Dense(512,activation='relu')(x)\n",
    "preds=tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')(x) #final layer with softmax activation\n",
    "model=tf.keras.Model(inputs=network.input,outputs=preds)\n",
    "\n",
    "model = tf.keras.utils.multi_gpu_model( model, 4) # before first epoch 689, second epoch 671. After: 837 seconds\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "count_n = 0\n",
    "with open('count_run.txt') as f:\n",
    "    count_n = int(f.readline())\n",
    "with open('count_run.txt', 'w') as f:\n",
    "    f.write(str(count_n+1) + '\\n')\n",
    "    \n",
    "mycallbacks = [tf.keras.callbacks.TensorBoard(log_dir='./runs/'+str(count_n), write_graph=True, write_grads=True,write_images=True)]\n",
    "# mycallbacks=[]\n",
    "print(count_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "591/591 [==============================] - 379s 642ms/step - loss: 3.3878 - acc: 0.2304 - val_loss: 1.9961 - val_acc: 0.4854\n",
      "Epoch 2/50\n",
      "591/591 [==============================] - 339s 574ms/step - loss: 2.5375 - acc: 0.3820 - val_loss: 1.4821 - val_acc: 0.6049\n",
      "Epoch 3/50\n",
      "591/591 [==============================] - 338s 572ms/step - loss: 2.2662 - acc: 0.4416 - val_loss: 1.3176 - val_acc: 0.6442\n",
      "Epoch 4/50\n",
      "591/591 [==============================] - 340s 575ms/step - loss: 2.1131 - acc: 0.4712 - val_loss: 1.2330 - val_acc: 0.6722\n",
      "Epoch 5/50\n",
      "591/591 [==============================] - 340s 575ms/step - loss: 2.0140 - acc: 0.4952 - val_loss: 1.1815 - val_acc: 0.6831\n",
      "Epoch 6/50\n",
      "591/591 [==============================] - 338s 572ms/step - loss: 1.9165 - acc: 0.5165 - val_loss: 1.1164 - val_acc: 0.6936\n",
      "Epoch 7/50\n",
      "591/591 [==============================] - 341s 577ms/step - loss: 1.8516 - acc: 0.5309 - val_loss: 1.0306 - val_acc: 0.7216\n",
      "Epoch 8/50\n",
      "591/591 [==============================] - 341s 578ms/step - loss: 1.7874 - acc: 0.5482 - val_loss: 0.9990 - val_acc: 0.7274\n",
      "Epoch 9/50\n",
      "591/591 [==============================] - 341s 577ms/step - loss: 1.7386 - acc: 0.5579 - val_loss: 1.0230 - val_acc: 0.7189\n",
      "Epoch 10/50\n",
      "591/591 [==============================] - 341s 576ms/step - loss: 1.6953 - acc: 0.5679 - val_loss: 0.9386 - val_acc: 0.7422\n",
      "Epoch 11/50\n",
      "591/591 [==============================] - 343s 581ms/step - loss: 1.6469 - acc: 0.5794 - val_loss: 0.8944 - val_acc: 0.7530\n",
      "Epoch 12/50\n",
      "591/591 [==============================] - 340s 575ms/step - loss: 1.6073 - acc: 0.5873 - val_loss: 0.9101 - val_acc: 0.7515\n",
      "Epoch 13/50\n",
      "591/591 [==============================] - 339s 573ms/step - loss: 1.5759 - acc: 0.5968 - val_loss: 0.9366 - val_acc: 0.7463\n",
      "Epoch 14/50\n",
      "591/591 [==============================] - 340s 576ms/step - loss: 1.5331 - acc: 0.6037 - val_loss: 0.9224 - val_acc: 0.7453\n",
      "Epoch 15/50\n",
      "591/591 [==============================] - 340s 575ms/step - loss: 1.5071 - acc: 0.6108 - val_loss: 0.8817 - val_acc: 0.7537\n",
      "Epoch 16/50\n",
      "591/591 [==============================] - 339s 573ms/step - loss: 1.4802 - acc: 0.6164 - val_loss: 0.8368 - val_acc: 0.7670\n",
      "Epoch 17/50\n",
      "591/591 [==============================] - 339s 574ms/step - loss: 1.4507 - acc: 0.6245 - val_loss: 0.8418 - val_acc: 0.7673\n",
      "Epoch 18/50\n",
      "591/591 [==============================] - 339s 573ms/step - loss: 1.4273 - acc: 0.6289 - val_loss: 0.8182 - val_acc: 0.7736\n",
      "Epoch 19/50\n",
      "591/591 [==============================] - 339s 574ms/step - loss: 1.4100 - acc: 0.6335 - val_loss: 0.8073 - val_acc: 0.7770\n",
      "Epoch 20/50\n",
      "591/591 [==============================] - 340s 576ms/step - loss: 1.3778 - acc: 0.6405 - val_loss: 0.7893 - val_acc: 0.7807\n",
      "Epoch 21/50\n",
      "591/591 [==============================] - 339s 573ms/step - loss: 1.3508 - acc: 0.6474 - val_loss: 0.8048 - val_acc: 0.7811\n",
      "Epoch 22/50\n",
      "591/591 [==============================] - 343s 580ms/step - loss: 1.3364 - acc: 0.6499 - val_loss: 0.7988 - val_acc: 0.7837\n",
      "Epoch 23/50\n",
      "591/591 [==============================] - 339s 573ms/step - loss: 1.3164 - acc: 0.6544 - val_loss: 0.8038 - val_acc: 0.7819\n",
      "Epoch 24/50\n",
      "591/591 [==============================] - 339s 574ms/step - loss: 1.2995 - acc: 0.6593 - val_loss: 0.7943 - val_acc: 0.7830\n",
      "Epoch 25/50\n",
      "591/591 [==============================] - 340s 576ms/step - loss: 1.2730 - acc: 0.6666 - val_loss: 0.7373 - val_acc: 0.7958\n",
      "Epoch 26/50\n",
      "591/591 [==============================] - 338s 572ms/step - loss: 1.2577 - acc: 0.6677 - val_loss: 0.7500 - val_acc: 0.7926\n",
      "Epoch 27/50\n",
      "591/591 [==============================] - 340s 575ms/step - loss: 1.2410 - acc: 0.6726 - val_loss: 0.7422 - val_acc: 0.7955\n",
      "Epoch 28/50\n",
      "591/591 [==============================] - 341s 577ms/step - loss: 1.2248 - acc: 0.6742 - val_loss: 0.7428 - val_acc: 0.7974\n",
      "Epoch 29/50\n",
      "591/591 [==============================] - 339s 574ms/step - loss: 1.2074 - acc: 0.6800 - val_loss: 0.7494 - val_acc: 0.7961\n",
      "Epoch 30/50\n",
      "591/591 [==============================] - 339s 574ms/step - loss: 1.1953 - acc: 0.6820 - val_loss: 0.7285 - val_acc: 0.8005\n",
      "Epoch 31/50\n",
      "591/591 [==============================] - 341s 577ms/step - loss: 1.1820 - acc: 0.6859 - val_loss: 0.7646 - val_acc: 0.7910\n",
      "Epoch 32/50\n",
      "591/591 [==============================] - 340s 575ms/step - loss: 1.1620 - acc: 0.6923 - val_loss: 0.7322 - val_acc: 0.7993\n",
      "Epoch 33/50\n",
      "591/591 [==============================] - 337s 570ms/step - loss: 1.1458 - acc: 0.6954 - val_loss: 0.7458 - val_acc: 0.7979\n",
      "Epoch 34/50\n",
      "591/591 [==============================] - 341s 578ms/step - loss: 1.1375 - acc: 0.6954 - val_loss: 0.7121 - val_acc: 0.8056\n",
      "Epoch 35/50\n",
      "591/591 [==============================] - 337s 571ms/step - loss: 1.1215 - acc: 0.6995 - val_loss: 0.7358 - val_acc: 0.7991\n",
      "Epoch 36/50\n",
      "591/591 [==============================] - 341s 576ms/step - loss: 1.1077 - acc: 0.7034 - val_loss: 0.7315 - val_acc: 0.7997\n",
      "Epoch 37/50\n",
      "591/591 [==============================] - 340s 575ms/step - loss: 1.0955 - acc: 0.7067 - val_loss: 0.7414 - val_acc: 0.8021\n",
      "Epoch 38/50\n",
      "591/591 [==============================] - 340s 575ms/step - loss: 1.0927 - acc: 0.7058 - val_loss: 0.7275 - val_acc: 0.8037\n",
      "Epoch 39/50\n",
      "591/591 [==============================] - 343s 581ms/step - loss: 1.0783 - acc: 0.7098 - val_loss: 0.7304 - val_acc: 0.8035\n",
      "Epoch 40/50\n",
      "591/591 [==============================] - 341s 576ms/step - loss: 1.0595 - acc: 0.7148 - val_loss: 0.7230 - val_acc: 0.8050\n",
      "Epoch 41/50\n",
      "591/591 [==============================] - 340s 576ms/step - loss: 1.0551 - acc: 0.7147 - val_loss: 0.7307 - val_acc: 0.8031\n",
      "Epoch 42/50\n",
      "591/591 [==============================] - 339s 574ms/step - loss: 1.0392 - acc: 0.7200 - val_loss: 0.7349 - val_acc: 0.8056\n",
      "Epoch 43/50\n",
      "591/591 [==============================] - 339s 574ms/step - loss: 1.0242 - acc: 0.7221 - val_loss: 0.7139 - val_acc: 0.8089\n",
      "Epoch 44/50\n",
      "591/591 [==============================] - 338s 572ms/step - loss: 1.0151 - acc: 0.7256 - val_loss: 0.7382 - val_acc: 0.8004\n",
      "Epoch 45/50\n",
      "591/591 [==============================] - 338s 571ms/step - loss: 1.0131 - acc: 0.7270 - val_loss: 0.7279 - val_acc: 0.8067\n",
      "Epoch 46/50\n",
      "591/591 [==============================] - 343s 580ms/step - loss: 0.9999 - acc: 0.7284 - val_loss: 0.7253 - val_acc: 0.8070\n",
      "Epoch 47/50\n",
      "591/591 [==============================] - 340s 575ms/step - loss: 0.9869 - acc: 0.7314 - val_loss: 0.7286 - val_acc: 0.8083\n",
      "Epoch 48/50\n",
      "591/591 [==============================] - 342s 579ms/step - loss: 0.9763 - acc: 0.7340 - val_loss: 0.7223 - val_acc: 0.8095\n",
      "Epoch 49/50\n",
      "591/591 [==============================] - 337s 570ms/step - loss: 0.9695 - acc: 0.7371 - val_loss: 0.7120 - val_acc: 0.8090\n",
      "Epoch 50/50\n",
      "503/591 [========================>.....] - ETA: 42s - loss: 0.9580 - acc: 0.7388"
     ]
    }
   ],
   "source": [
    "# model.fit_generator(train_generator_from_folder, epochs=5, workers=16, validation_data=test_generator, callbacks=mycallbacks)\n",
    "model.fit_generator(train_generator, \n",
    "                    epochs=50, \n",
    "#                     max_queue_size=100, \n",
    "                    use_multiprocessing=True, \n",
    "                    workers=16, \n",
    "                    validation_data=test_generator, \n",
    "                    callbacks=mycallbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cake_recognition_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = list()\n",
    "all_true = list()\n",
    "for i in range(len(test_generator)):\n",
    "    images, labels = test_generator.__getitem__(i)\n",
    "    predictions = model.predict(images)\n",
    "    all_predictions.extend(predictions)\n",
    "    all_true.extend(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(all_predictions))\n",
    "print(all_predictions[0])\n",
    "max_pred = [np.argmax(p) for p in all_predictions]\n",
    "max_true = [np.argmax(p) for p in all_true]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_correct = 0\n",
    "for a, b in zip(max_pred, max_true):\n",
    "    if a==b:\n",
    "        total_correct += 1\n",
    "print(total_correct / len(all_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "#     classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20,20))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "# class_names = iris.target_names\n",
    "class_names = test_generator.label_names\n",
    "# Plot non-normalized confusion matrix\n",
    "fig, ax = plot_confusion_matrix(max_pred, max_true, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# # Plot normalized confusion matrix\n",
    "# plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
    "#                       title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('confusion_matrix.png')\n",
    "fig.savefig('confusion_matrix.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = test_generator.__getitem__(0)\n",
    "predictions = model.predict(images)\n",
    "for p, true in zip(predictions,labels):\n",
    "    print(p.max(), \" index \", p.argmax(), ' actual activation: ', p[true.argmax()])\n",
    "\n",
    "\n",
    "for index in range(32):\n",
    "    plt.imshow(images[index]*255)\n",
    "    plt.show()\n",
    "    print(train_generator.label_names[labels[index].argmax()])\n",
    "    print(train_generator.label_names[predictions[index].argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    images, labels = test_generator.__getitem__(0)\n",
    "    predictions = model.predict(images)\n",
    "    for p, true in zip(predictions,labels):\n",
    "#         print(p.max(), \" index \", p.argmax(), ' actual activation: ', p[true.argmax()])\n",
    "        if p[0] > 0.2:\n",
    "            plt.imshow(images[index]*255)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
